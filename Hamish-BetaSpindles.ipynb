{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project to classify Beta Spindles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the data:"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load all the data\n",
    "def load_all_data(filename):\n",
    "    import numpy\n",
    "    \n",
    "    a = np.load(filename)\n",
    "    d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "filename = '<FILENAME>' #fill in the path/filename you saved the data at\n",
    "X = load_all_data(filename)[0]#I think this is the array order that I used\n",
    "y = load_all_data(filename)[1]\n",
    "\n",
    "IDs = load_all_data(filename)[2]\n",
    "#we ignore age for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "test_size = 0.2\n",
    "gss = GroupShuffleSplit(1, test_size)#you can look up specifics in the sklearn documentation\n",
    "\n",
    "for train,test in gss.split(X, y, IDs):\n",
    "    X_train = X[train]\n",
    "    y_train = y[train]\n",
    "    IDs_train = IDs[train]\n",
    "    \n",
    "    X_test = X[test]\n",
    "    y_test = y[test]\n",
    "    IDs_test = IDs[test]\n",
    "\n",
    "fileoutname = '<FILEOUTNAME>'# fill in the path/filename you want to save the data at.\n",
    "#Now save the train and test data\n",
    "np.savez(fileoutname,X_train, y_train, X_test, y_test,IDs_train,IDs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont look at the Test data again untill we are happy about the results of the model development and are ready to tests its generalizability to independent data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the same method to extract your development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train,test in gss.split(X_train, y_train, IDs_train):\n",
    "#etc fill in yourself :-)\n",
    "#call the development set X_dev, y_dev, IDs_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use np.expand_dims to add an extra dimension at the end of the matrix (because the neural networks using \n",
    "Conv2D need matrices of samples x height x width x channels). Y needs to be transformed 'to categorical' for the output that we want. We don't only want to have 1 or 0 as output. We want to know how sure the model is about this, in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2 = np.expand_dims(X_train2)#etc\n",
    "y_train2 = to_categorical(y_train2)#etc --> same for the development X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next would be to build a Convolutional Neural Network. Lets start with a (relatively) simple one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simpleCNN(self, units = 10):\n",
    "    \n",
    "    import keras\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Conv2D\n",
    "    from keras.models import Model, Input\n",
    "    \n",
    "    inp =  Input(shape = self.shape[1:], name='inp')\n",
    "    #layer 1\n",
    "    x = Conv2D(units, kernel_size=(1,1), strides = (1,1), activation='relu', data_format='channels_last')(inp)  \n",
    "    #layer 2\n",
    "    x = Conv2D(units, kernel_size=(2,2), strides = (1,1), activation='relu', data_format='channels_last')(x)  \n",
    "    #layer 3 \n",
    "    x = Flatten()(x)\n",
    "    #layer4\n",
    "    out = Dense(2, activation='softmax',name='out')(x)\n",
    "    \n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adamax\n",
    "adamax = Adamax(lr=3e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0); #3e-4; 2e-3 is a default.\n",
    "\n",
    "model.compile(optimizer=adamax, loss='categorical_crossentropy', metrics=['accuracy', matthews_correlation])\n",
    "model.fit(X_train2, y_train2, epochs=epochs, batch_size=batch_size, verbose = 1, validation_data = (X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suggest you start playing around with this. Read the sklearn and Keras documentation and try out several possibilities, e.g. more/less layers, dropout, different outputs, different optimizers. You could also choose to use less data if it takes to long for your computer to make the calculations. There are lots of things to play around with, so I look forward to see what you come up with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
